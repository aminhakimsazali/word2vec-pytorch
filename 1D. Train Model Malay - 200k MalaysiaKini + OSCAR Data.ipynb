{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bfff64",
   "metadata": {},
   "source": [
    "# Build DataLoader for Malaysia Kini data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f032a3",
   "metadata": {},
   "source": [
    "## Read data all Malaysia Kini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8044f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f5d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/malaysia-kini/06_MalaysiaKini & Awani Articles 2022.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fb7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116030 entries, 0 to 116029\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Target       116030 non-null  object\n",
      " 1   Source       116030 non-null  object\n",
      " 2   Text Length  116030 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db9271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4686192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text Length'].sum()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96521bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df['Target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fedb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.extend( df['Source'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f649395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeries = pd.Series(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2708bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeries.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6028a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeries.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6818e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dfSeries = shuffle(dfSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4201e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeries.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e72af",
   "metadata": {},
   "source": [
    "## Read data OSCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bf22215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOSCAR2109 = pd.read_csv(\"data/OSCAR 2109.csv\")\n",
    "dfOSCAR2109.columns = [\"Index\", \"Text\", \"Time\"]\n",
    "\n",
    "dfOSCAR2201 = pd.read_excel(\"data/OSCAR 2201.xlsx\")\n",
    "dfOSCAR2201.columns = [\"Index\", \"Text\"]\n",
    "\n",
    "dfOSCAR2301 = pd.read_csv(\"data/OSCAR 2301.csv\")\n",
    "dfOSCAR2301.columns = [\"Index\", \"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eba30c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aminh\\AppData\\Local\\Temp\\ipykernel_17376\\1807688136.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfSeries = dfSeries.append(dfOSCAR2109.Text)\n",
      "C:\\Users\\aminh\\AppData\\Local\\Temp\\ipykernel_17376\\1807688136.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfSeries = dfSeries.append(dfOSCAR2201.Text)\n",
      "C:\\Users\\aminh\\AppData\\Local\\Temp\\ipykernel_17376\\1807688136.py:3: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfSeries = dfSeries.append(dfOSCAR2301.Text)\n"
     ]
    }
   ],
   "source": [
    "dfSeries = dfSeries.append(dfOSCAR2109.Text)\n",
    "dfSeries = dfSeries.append(dfOSCAR2201.Text)\n",
    "dfSeries = dfSeries.append(dfOSCAR2301.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c22b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dfSeries = shuffle(dfSeries)\n",
    "\n",
    "dfSeries.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f43bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeries.to_excel(\"data/malaysia-kini/07_MalaysiaKini & Awani Articles 2022 + OSCAR.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35896c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Sekiranya kayap menyerang bahagian muka pula, ...\n",
       "1         katanya, mou tu terutamanya membabitkan kerjas...\n",
       "2         Jangan terlalu gigih melaksanakan umrah sunat ...\n",
       "3         lain-lain -- seperti penyampaian kandungan vid...\n",
       "4         \"Oleh itu, saya berhenti sementara daripada me...\n",
       "                                ...                        \n",
       "436891    malah ia berubah secara radikal drpd bentuk as...\n",
       "436892    ketua pegawai eksekutif ktmb, mohd rani hisham...\n",
       "436893    \"namun apabila balik rumah, baharulah saya bat...\n",
       "436894    \"kami bagaimanapun mengekalkan harga sasaran f...\n",
       "436895    memandangkan populasi muslim global diramalkan...\n",
       "Length: 436896, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a09dc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp = pd.read_excel(\"data/malaysia-kini/07_MalaysiaKini & Awani Articles 2022 + OSCAR.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d04b908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sekiranya kayap menyerang bahagian muka pula, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>katanya, mou tu terutamanya membabitkan kerjas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jangan terlalu gigih melaksanakan umrah sunat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>lain-lain -- seperti penyampaian kandungan vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Oleh itu, saya berhenti sementara daripada me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436891</th>\n",
       "      <td>436891</td>\n",
       "      <td>malah ia berubah secara radikal drpd bentuk as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436892</th>\n",
       "      <td>436892</td>\n",
       "      <td>ketua pegawai eksekutif ktmb, mohd rani hisham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436893</th>\n",
       "      <td>436893</td>\n",
       "      <td>\"namun apabila balik rumah, baharulah saya bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436894</th>\n",
       "      <td>436894</td>\n",
       "      <td>\"kami bagaimanapun mengekalkan harga sasaran f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436895</th>\n",
       "      <td>436895</td>\n",
       "      <td>memandangkan populasi muslim global diramalkan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436896 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                                  0\n",
       "0                0  Sekiranya kayap menyerang bahagian muka pula, ...\n",
       "1                1  katanya, mou tu terutamanya membabitkan kerjas...\n",
       "2                2  Jangan terlalu gigih melaksanakan umrah sunat ...\n",
       "3                3  lain-lain -- seperti penyampaian kandungan vid...\n",
       "4                4  \"Oleh itu, saya berhenti sementara daripada me...\n",
       "...            ...                                                ...\n",
       "436891      436891  malah ia berubah secara radikal drpd bentuk as...\n",
       "436892      436892  ketua pegawai eksekutif ktmb, mohd rani hisham...\n",
       "436893      436893  \"namun apabila balik rumah, baharulah saya bat...\n",
       "436894      436894  \"kami bagaimanapun mengekalkan harga sasaran f...\n",
       "436895      436895  memandangkan populasi muslim global diramalkan...\n",
       "\n",
       "[436896 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8c79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9bab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c78a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87d4d2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07ba6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.dataloader import get_dataloader_and_vocab\n",
    "from utils.trainer import Trainer\n",
    "from utils.helper import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    get_lr_scheduler,\n",
    "    save_config,\n",
    "    save_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30c24e",
   "metadata": {},
   "source": [
    "## 1: Configuration File & Delete existing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea3209f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model_name': 'skipgram',\n",
    "#  'dataset': 'WikiText2',\n",
    "#  'data_dir': 'data/',\n",
    " 'train_batch_size': 96,\n",
    " 'val_batch_size': 96,\n",
    " 'shuffle': True,\n",
    " 'optimizer': 'Adam',\n",
    " 'learning_rate': 0.025,\n",
    " 'epochs': 5,\n",
    " 'train_steps': None,\n",
    " 'val_steps': None,\n",
    " 'checkpoint_frequency': None,\n",
    " 'model_dir': 'weights/skipgram_MalaysiaKini_200k'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96470bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists(config['model_dir']):\n",
    "#     os.removedirs(config['model_dir'])\n",
    "    shutil.rmtree(config['model_dir'])\n",
    "\n",
    "#Create model directory\n",
    "os.makedirs(config[\"model_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86470000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def to_map_style_dataset(iter_data):\n",
    "    r\"\"\"Convert iterable-style dataset to map-style dataset.\n",
    "\n",
    "    args:\n",
    "        iter_data: An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> from torchtext.datasets import IMDB\n",
    "        >>> from torchtext.data import to_map_style_dataset\n",
    "        >>> train_iter = IMDB(split='train')\n",
    "        >>> train_dataset = to_map_style_dataset(train_iter)\n",
    "        >>> file_name = '.data/EnWik9/enwik9'\n",
    "        >>> data_iter = to_map_style_dataset(open(file_name,'r'))\n",
    "    \"\"\"\n",
    "\n",
    "    # Inner class to convert iterable-style to map-style dataset\n",
    "    class _MapStyleDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, iter_data):\n",
    "            # TODO Avoid list issue #1296\n",
    "#             self._data = list(iter_data)\n",
    "            self._data =  iter_data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self._data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data[idx]\n",
    "\n",
    "    return _MapStyleDataset(iter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17864a44",
   "metadata": {},
   "source": [
    "## 2: Read data and create DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7224be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MalaysiaKiniData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        #load data\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #return exact data\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return data length\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d06019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import collate_skipgram, collate_cbow, build_vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "\n",
    "def get_dataloader_and_vocab_malay_dataPath(\n",
    "    model_name, ds_path, ds_column, batch_size, shuffle, vocab=None, \n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    function use data path as parameter\n",
    "    \"\"\"\n",
    "        \n",
    "    df = pd.read_csv(ds_path)\n",
    "    df = df.dropna(how=\"any\")\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def yield_sentences():\n",
    "        for sent in df[ds_column]:\n",
    "            yield sent\n",
    "\n",
    "    sentences_generator = yield_sentences()\n",
    "\n",
    "    data_iter = MalaysiaKiniData(df[ds_column])\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    \n",
    "    if not vocab:\n",
    "        vocab = build_vocab(sentences_generator, tokenizer)\n",
    "        \n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "    if model_name == \"cbow\":\n",
    "        collate_fn = collate_cbow\n",
    "    elif model_name == \"skipgram\":\n",
    "        collate_fn = collate_skipgram\n",
    "    else:\n",
    "        raise ValueError(\"Choose model from: cbow, skipgram\")\n",
    "    print(\"Vocab size: \", len(vocab.get_stoi()))\n",
    "    dataloader = DataLoader(\n",
    "        data_iter,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
    "    )\n",
    "    return dataloader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73c0b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_and_vocab_malay_dataset(\n",
    "    model_name, data, batch_size, shuffle, vocab=None,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    function use data as parameter\n",
    "    \"\"\"\n",
    "\n",
    "#     if ds_path is not None:\n",
    "#         df = pd.read_csv(ds_path)\n",
    "#         df = df.dropna(how=\"any\")\n",
    "#     else:\n",
    "#     df = data\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def yield_sentences():\n",
    "        for sent in data:\n",
    "            yield sent\n",
    "\n",
    "    sentences_generator = yield_sentences()\n",
    "\n",
    "    data_iter = MalaysiaKiniData(data)\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    \n",
    "    if not vocab:\n",
    "        vocab = build_vocab(sentences_generator, tokenizer)\n",
    "        \n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "    if model_name == \"cbow\":\n",
    "        collate_fn = collate_cbow\n",
    "    elif model_name == \"skipgram\":\n",
    "        collate_fn = collate_skipgram\n",
    "    else:\n",
    "        raise ValueError(\"Choose model from: cbow, skipgram\")\n",
    "    print(\"Vocab size: \", len(vocab.get_stoi()))\n",
    "    dataloader = DataLoader(\n",
    "        data_iter,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
    "    )\n",
    "    return dataloader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff5ba1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10458\n",
      "Vocabulary size: 10458\n"
     ]
    }
   ],
   "source": [
    "#Load train dataset\n",
    "\n",
    "train_dataloader, vocab = get_dataloader_and_vocab_malay_dataset(\n",
    "    model_name=config[\"model_name\"],\n",
    "    data = dfSeries,\n",
    "#     ds_path = \"data/malaysia-kini/train.csv\",\n",
    "#     ds_column = \"Target\",\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "#     shuffle=config[\"shuffle\"],\n",
    "    shuffle=False,\n",
    "    vocab=None,\n",
    ")\n",
    "\n",
    "#Get vocab size\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f421fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10458\n"
     ]
    }
   ],
   "source": [
    "#Load Validation dataset\n",
    "val_dataloader, _ = get_dataloader_and_vocab_malay_dataset(\n",
    "    model_name=config[\"model_name\"],\n",
    "        data = dfSeries,\n",
    "\n",
    "#     ds_path = \"data/malaysia-kini/valid.csv\",\n",
    "#     ds_column = \"Target\",\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "#     shuffle=config[\"shuffle\"],\n",
    "    shuffle=False,\n",
    "    vocab=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "713e27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10458\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n"
     ]
    }
   ],
   "source": [
    "#Get vocab size\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "#Get model config\n",
    "model_class = get_model_class(config[\"model_name\"])\n",
    "model = model_class(vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Model parameters : optimizer, learning rate\n",
    "optimizer_class = get_optimizer_class(config[\"optimizer\"])\n",
    "optimizer = optimizer_class(model.parameters(), lr=config[\"learning_rate\"])\n",
    "lr_scheduler = get_lr_scheduler(optimizer, config[\"epochs\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7090b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training on CUDA if CUDA available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01b0a761",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#Finish training\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[1;32m~\\Documents\\word2vec-pytorch-main\\utils\\trainer.py:45\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_epoch()\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Train Loss=\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m, Val Loss=\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     49\u001b[0m                 epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m             )\n\u001b[0;32m     54\u001b[0m         )\n",
      "File \u001b[1;32m~\\Documents\\word2vec-pytorch-main\\utils\\trainer.py:72\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[0;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m running_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch1.13\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=config[\"epochs\"],\n",
    "    train_dataloader=train_dataloader,\n",
    "    train_steps=config[\"train_steps\"],\n",
    "    val_dataloader=val_dataloader,\n",
    "    val_steps=config[\"val_steps\"],\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    checkpoint_frequency=config[\"checkpoint_frequency\"],\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "    model_dir=config[\"model_dir\"],\n",
    "    model_name=config[\"model_name\"],\n",
    ")\n",
    "\n",
    "#Finish training\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "save_vocab(vocab, config[\"model_dir\"])\n",
    "save_config(config, config[\"model_dir\"])\n",
    "print(\"Model artifacts saved to folder:\", config[\"model_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55494370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
